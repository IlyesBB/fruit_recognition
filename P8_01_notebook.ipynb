{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importations et définitions de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 13:53:28.763175: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-29 13:53:28.763222: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import json\n",
    "import boto.s3, boto.s3.key\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "import numpy as np\n",
    "import os\n",
    "from pyspark.sql import Row\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/29 13:54:00 WARN Utils: Your hostname, DESKTOP-OL5H896 resolves to a loopback address: 127.0.1.1; using 172.29.208.25 instead (on interface eth0)\n",
      "22/04/29 13:54:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ilyes/OC/P8/venv/lib/python3.9/site-packages/pyspark/spark-distribution/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/29 13:54:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# sc: Contexte Spark\n",
    "sc = SparkContext('local')\n",
    "# spark: Session Spark\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définie les variables principales, notamment le nom du bucket et le chemin vers les images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket: Nom du bucket s3\n",
    "bucket = 'ib-ds-p8'\n",
    "# prefix_bucket: Préfixe pour les chemins de fichiers sur le serveur S3\n",
    "prefix_bucket = 's3a://' + bucket + '/'\n",
    "# data_dir: Répertoire contenant les dossiers d'entraînement et de test, notamment\n",
    "data_dir = 'archive/fruits-360_dataset/fruits-360'\n",
    "# train_dir: Répertoire contenant les dossiers d'entraînement pour chaque fruit\n",
    "train_dir = data_dir + '/Training'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée un RDD à partir des chemins des images, qu'on récupère à l'aide de boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucket)\n",
    "# file_paths: Chemins des images des fruits/légumes.\n",
    "file_paths = [obj.key for obj in my_bucket.objects.filter(Prefix=(train_dir+'/'))]\n",
    "# n_images: Nombre d'images à traiter\n",
    "n_images = 10\n",
    "file_paths = file_paths[:n_images]\n",
    "# rdd: Contient la base de données des chemins sous forme distribuée\n",
    "rdd = sc.parallelize(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_names = [path.split('/')[-2] for path in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_client: Client s3 qui va nous permettre de lire les images\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On va pré-traiter 5 images\n"
     ]
    }
   ],
   "source": [
    "print(\"On va pré-traiter %d images\" % len(file_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour featuriser les images, on va utiliser le réseau de neurones ResNet50 sans sa la dernière couche. Pour cela, on va enregistrer les poids du réseau, de façon à les télécharger une seule fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 13:54:33.220872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-29 13:54:33.231527: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:54:33.231652: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:54:33.231716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:54:33.231775: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:54:33.231832: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:54:33.231887: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:54:33.231941: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:54:33.231997: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:54:33.232007: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-29 13:54:33.243774: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# model: Contient le modèle ResNet50, sans la dernière couche\n",
    "model = ResNet50(include_top=False)\n",
    "\n",
    "# bc_model_weights: Variable partagée par tous les clusters\n",
    "bc_model_weights = sc.broadcast(model.get_weights())\n",
    "\n",
    "def model_fn():\n",
    "  \"\"\"Fonction permettant d'accéder au modèle ResNet50 sans re-télécharger les poids\"\"\"\n",
    "  model = ResNet50(weights=None, include_top=False)\n",
    "  model.set_weights(bc_model_weights.value)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On configure Spark pour transférer les bases de données de Pandas à Pyspark: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit les fonctions servant à la lecture des fichiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_bytes_local(file_path):\n",
    "    \"\"\"Sert uniquement à tester le fonctionnement du prétraitement sans utiliser le serveur S3\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        body = f.read()\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_bytes(file_path):\n",
    "    \"\"\"Sert à lire le contenu d'un fichier dans le bucket ib-ds-p8\"\"\"\n",
    "    # bucket: Nom du bucket contenant les données\n",
    "    bucket = 'ib-ds-p8'\n",
    "    # s3_client: Client s3 \n",
    "    s3_client = boto3.client('s3')\n",
    "    # bytesio: Flux de bits\n",
    "    bytesio = BytesIO()\n",
    "    # Lit le contenu du fichier de chemin file_path dans bytesio\n",
    "    s3_client.download_fileobj(bucket, file_path, bytesio)\n",
    "    # Se place au début du contenu pour la lecture\n",
    "    bytesio.seek(0)\n",
    "    return bytesio.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On parcours le RDD, en remplacant les noms de fichiers par le contenu des fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd: Contient à présent la base de données du contenu de chaque image\n",
    "rdd = rdd.map(path_to_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit les fonctions pour décoder le contenu des fichiers, et les featuriser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_array(bytes_str):\n",
    "    \"\"\"Transforme une liste de bits en matrice RGB d'une image 100x100px\"\"\"\n",
    "    # arr: Transformation de la suite de bits en matrice \n",
    "    arr = np.frombuffer(bytes_str, dtype=np.uint8)\n",
    "    # arr: Transformation de la matrice au format RGB\n",
    "    arr = cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "    arr = arr.astype(np.float32)  # cast en flottant\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_series(model, content_series):\n",
    "  \"\"\"Featurise une série d'images grâce au modèle\"\"\"\n",
    "  # input: Contient la série des matrices d'images\n",
    "  input = content_series.map(bytes_to_array)\n",
    "  # On va formater les matrices d'images en entrées pour ResNet50\n",
    "  input = np.array(input.to_list(), ndmin=4)\n",
    "  input = preprocess_input(input)\n",
    "  # preds: Contient les images featurisées\n",
    "  preds = model.predict(input)\n",
    "  preds = pd.Series([pred.flatten() for pred in preds])\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilyes/OC/P8/venv/lib/python3.9/site-packages/pyspark/sql/pandas/functions.py:389: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "  \"\"\"\n",
    "  Itérateur permettant de Pré-traiter l'ensemble des séries d'image\n",
    "  :param content_series_iter: Itérateur sur des séries d'images\n",
    "  \"\"\"\n",
    "  # model: Contient le modèle enregistré\n",
    "  model = model_fn()\n",
    "  # On parcours les séries d'images en les featurisant\n",
    "  for content_series in content_series_iter:\n",
    "    yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour appliquer ces transformation, on repasse en format dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# data: RDD des images sous forme de dataframe à une colonne\n",
    "data = rdd.map(Row).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: Dataframe des images featurisées\n",
    "data = data.withColumn('_1', featurize_udf('_1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réduction de dimension: SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos données sont des images. La featurisation produit donc des vecteurs creux. Un outils adapté est donc la décomposition par valeurs singulières."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par obtenir la taille d'un vecteur d'une image, en appliquant la featurisation sur une image d'exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file: Fichier de l'image d'exemple\n",
    "test_file = 'archive/fruits-360_dataset/fruits-360/Training/Apple_Braeburn/0_100.jpg'\n",
    "# img_bytes: Contenu du fichier de l'image d'exemple\n",
    "img_bytes = path_to_bytes(test_file)\n",
    "# features: Vecteur des features de l'image d'exemple\n",
    "features = featurize_series(model, pd.Series([img_bytes]))\n",
    "# size_vect: Nombre de features\n",
    "size_vect = len(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La featurisation retourne des vecteurs de taille 32768\n"
     ]
    }
   ],
   "source": [
    "print('La featurisation retourne des vecteurs de taille %d' % size_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que la featurisation n'a pas diminué la taille de nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rdd = data.rdd.map(lambda vector: list(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On eregistre nos données featurisées dans le cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[13] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour faire la décomposition par valeur singulière, on va utiliser la classe RowMatrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = RowMatrix(data_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on prend un nombre faible de de composantes, pour les besoins de la démonstration. Mais pour contrôler la représentativité de la réduction de dimension, il faudrai prendre le nombre de composantes maximal. De cette façon, on aurais toutes les valeurs singulières et on connaîtrais ainsi le pourcentage de la variance du système représentée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_comps: Nombre de variables après réduction de dimension\n",
    "n_comps = min([n_images, size_vect, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 13:54:58.007844: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:54:58.007895: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-04-29 13:55:00.447345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-29 13:55:00.447511: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:/home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:55:00.447585: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:/home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:55:00.447673: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:/home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:55:00.447778: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:/home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:55:00.447911: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:/home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:55:00.448075: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:/home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:55:00.448203: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:/home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:55:00.448333: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:/home/ilyes/OC/P8/venv/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2022-04-29 13:55:00.448378: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-29 13:55:00.448731: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "22/04/29 13:55:04 WARN RowMatrix: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n",
      "22/04/29 13:55:04 WARN InstanceBuilder$NativeARPACK: Failed to load implementation from:dev.ludovic.netlib.arpack.JNIARPACK\n",
      "22/04/29 13:55:06 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/04/29 13:55:06 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/04/29 13:55:08 WARN RowMatrix: The input data was not directly cached, which may hurt performance if its parent RDDs are also uncached.\n"
     ]
    }
   ],
   "source": [
    "svd = mat.computeSVD(n_comps, computeU=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enregistre les résultats dans le S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour pouvoir accéder à la région eu-west-3:\n",
    "os.environ['BOTO_USE_ENDPOINT_HEURISTICS'] = 'True'\n",
    "# conn: Connexion à la région eu-west-\n",
    "conn = boto.s3.connect_to_region(\"eu-west-3\")\n",
    "# conn_bucket: Connexion au bucket ib-ds-p8\n",
    "conn_bucket = conn.get_bucket(\"ib-ds-p8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enregistre la nouvelle matrice de nos données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilyes/OC/P8/venv/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key: Fichier qui va contenir la matrice de nos données\n",
    "key = boto.s3.key.Key(conn_bucket, \"svd_U.csv\")\n",
    "key.set_contents_from_string(svd.U.rows.map(Row).toDF().toPandas().to_csv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et on enregistre nos valeurs singulières:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key: Fichier qui va contenir les valeurs singulières\n",
    "key = boto.s3.key.Key(conn_bucket, \"svd_s.txt\")\n",
    "key.set_contents_from_string(json.dumps(svd.s.toArray().tolist(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enregistre aussi les catégories attendues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key: Fichier qui va contenir les valeurs singulières\n",
    "key = boto.s3.key.Key(conn_bucket, \"categories.txt\")\n",
    "key.set_contents_from_string(json.dumps(fruit_names, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut à présent lire les résultats enregistrés dans notre serveur S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données featurisées et de dimensions réduites:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[-0.4138718114327048,-0.9093679869284459]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[-0.4588569134322118,0.20633828442354954]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[-0.4580016557118478,0.23349066398283208]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[-0.4509786456302524,0.21738387605356027]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[-0.4527838350113478,0.1694129186208554]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                         _1\n",
       "0           0  [-0.4138718114327048,-0.9093679869284459]\n",
       "1           1  [-0.4588569134322118,0.20633828442354954]\n",
       "2           2  [-0.4580016557118478,0.23349066398283208]\n",
       "3           3  [-0.4509786456302524,0.21738387605356027]\n",
       "4           4   [-0.4527838350113478,0.1694129186208554]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytesio = BytesIO()\n",
    "svd_u = 'svd_U.csv'\n",
    "s3_client.download_fileobj(bucket, svd_u, bytesio)\n",
    "bytesio.seek(0)\n",
    "import pandas as pd\n",
    "df = pd.read_csv(bytesio)\n",
    "print(\"Données featurisées et de dimensions réduites:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs singulières:\n",
      "[821.5190352618974, 176.44595993169142]\n"
     ]
    }
   ],
   "source": [
    "bytesio = BytesIO()\n",
    "svd_s = 'svd_s.txt'\n",
    "s3_client.download_fileobj(bucket, svd_s, bytesio)\n",
    "bytesio.seek(0)\n",
    "svalues = json.loads(bytesio.read().decode('utf-8'))\n",
    "print(\"Valeurs singulières:\")\n",
    "print(svalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: attendues\n",
      "['Apple_Braeburn', 'Apple_Braeburn', 'Apple_Braeburn', 'Apple_Braeburn', 'Apple_Braeburn']\n"
     ]
    }
   ],
   "source": [
    "bytesio = BytesIO()\n",
    "fruit_names = 'categories.txt'\n",
    "s3_client.download_fileobj(bucket, fruit_names, bytesio)\n",
    "bytesio.seek(0)\n",
    "categories = json.loads(bytesio.read().decode('utf-8'))\n",
    "print(\"Categories: attendues\")\n",
    "print(categories)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12acfea4aee79fc130143472f57ab7acfcbea155596fd191f2593a3ac46fce26"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
